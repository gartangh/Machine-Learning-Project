{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wBnRNaGsMH3o"
   },
   "source": [
    "# Animal Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28lQpz2bMH3u"
   },
   "source": [
    "## 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8QVba2QtMH3z",
    "outputId": "24aae7e4-e2f3-4590-a903-b5b51860b0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes = 11\n",
      "0: bobcat\n",
      "1: chihuahua\n",
      "2: collie\n",
      "3: dalmatian\n",
      "4: german_shepherd\n",
      "5: leopard\n",
      "6: lion\n",
      "7: persian_cat\n",
      "8: siamese_cat\n",
      "9: tiger\n",
      "10: wolf\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import cv2, features, glob, helpers, os, pickle, sklearn, sys, time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.gaussian_process.kernels import ExpSineSquared\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel\n",
    "\n",
    "# define constants\n",
    "DATA_BASE_PATH = './'\n",
    "OUTPUT_PATH='./'\n",
    "\n",
    "DATA_TRAIN_PATH = os.path.join(DATA_BASE_PATH,'train')\n",
    "DATA_TEST_PATH = os.path.join(DATA_BASE_PATH,'test')\n",
    "\n",
    "FEATURE_BASE_PATH = os.path.join(OUTPUT_PATH,'features')\n",
    "FEATURE_TRAIN_PATH = os.path.join(FEATURE_BASE_PATH,'features_train')\n",
    "FEATURE_TEST_PATH = os.path.join(FEATURE_BASE_PATH,'features_test')\n",
    "\n",
    "PREDICTION_PATH = os.path.join(OUTPUT_PATH,'predictions')\n",
    "\n",
    "FILEPATTERN_DESCRIPTOR_TRAIN = os.path.join(FEATURE_TRAIN_PATH,'train_features_{}.pkl')\n",
    "FILEPATTERN_DESCRIPTOR_TEST = os.path.join(FEATURE_TEST_PATH,'test_features_{}.pkl')\n",
    "\n",
    "helpers.createPath(FEATURE_BASE_PATH)\n",
    "helpers.createPath(FEATURE_TRAIN_PATH)\n",
    "helpers.createPath(FEATURE_TEST_PATH)\n",
    "helpers.createPath(PREDICTION_PATH)\n",
    "\n",
    "# initialize variables\n",
    "folder_paths = glob.glob(os.path.join(DATA_TRAIN_PATH, '*'))\n",
    "label_strings = np.sort(np.array([os.path.basename(path) for path in folder_paths]))\n",
    "n_classes = label_strings.shape[0]\n",
    "print(f'Number of classes = {n_classes}')\n",
    "[print(f'{i}: {label_strings[i]}') for i in range(len(label_strings))]\n",
    "\n",
    "train_paths = dict((label_string, helpers.getImgPaths(os.path.join(DATA_TRAIN_PATH,label_string))) for label_string in label_strings)\n",
    "test_paths = helpers.getImgPaths(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6bVV3v4MH4P"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1F_s8pQyMH4T"
   },
   "outputs": [],
   "source": [
    "# remove background\n",
    "# mirror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "me77J1OcMH4f"
   },
   "source": [
    "## 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mwyUy5phMH4k"
   },
   "outputs": [],
   "source": [
    "def extract_features():\n",
    "    # descriptors\n",
    "    descriptor_dict={'daisy':features.extractDAISYCallback, # SIFT replacement, very fast, can be computed dense if necessary\n",
    "                     'orb':features.extractORBCallback, # another fast SIFT replacement, oriented BRIEF w. FAST keypoints  \n",
    "                     'freak':features.extractFREAKCallback, # biologically motivated descriptor\n",
    "                     'lucid':features.extractLUCIDCallback,  \n",
    "                     'vgg':features.extractVGGCallback, # Trained as proposed by VGG lab, don't confuse it with VGG-Net features\n",
    "                     'boost_desc':features.extractBoostDescCallback} # Image descriptor learned with boosting\n",
    "          \n",
    "    if features.checkForSIFT():\n",
    "        descriptor_dict['sift'] = features.extractSIFTCallback # One descriptor to rule them all, unfortunately patented\n",
    "                 \n",
    "    if features.checkForSURF():\n",
    "        descriptor_dict['surf'] = features.extractSURFCallback\n",
    "    \n",
    "    # train features\n",
    "    train_descriptor_dict = dict((key,value) for (key,value) in descriptor_dict.items() if not os.path.isfile(FILEPATTERN_DESCRIPTOR_TRAIN.format(key)))\n",
    "    if len(train_descriptor_dict) > 0: \n",
    "        train_features = []\n",
    "        train_labels = []\n",
    "    \n",
    "        train_features_by_descriptor = dict((key,[]) for (key,value) in train_descriptor_dict.items())\n",
    "    \n",
    "        for label_string in label_strings:\n",
    "            print(f'extracting train features for class {label_string}')\n",
    "            extracted_features = features.extractFeatures(train_paths[label_string], train_descriptor_dict, label_string)\n",
    "\n",
    "            # append descriptors of corresponding label to correct descriptor list \n",
    "            for key in train_features_by_descriptor.keys():\n",
    "                train_features_by_descriptor[key]+=extracted_features[key]\n",
    "  \n",
    "        for descriptor_key in train_features_by_descriptor.keys():\n",
    "            with open(FILEPATTERN_DESCRIPTOR_TRAIN.format(descriptor_key),'wb') as pkl_file_train:\n",
    "                pickle.dump(train_features_by_descriptor[descriptor_key], pkl_file_train, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # test features\n",
    "    test_descriptor_dict=dict((key,value) for (key,value) in descriptor_dict.items() if not os.path.isfile(FILEPATTERN_DESCRIPTOR_TEST.format(key)))\n",
    "    if len(test_descriptor_dict) > 0: \n",
    "        test_features = []\n",
    "    \n",
    "        print('extracting test features:')\n",
    "        test_features_by_descriptor = features.extractFeatures(test_paths,test_descriptor_dict, None) \n",
    "    \n",
    "        for descriptor_key in test_features_by_descriptor.keys():\n",
    "            with open(FILEPATTERN_DESCRIPTOR_TEST.format(descriptor_key),'wb') as pkl_file_test:\n",
    "                pickle.dump(test_features_by_descriptor[descriptor_key], pkl_file_test, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "def load_pkl_files(descriptors_desired, codebook_sizes):\n",
    "    print('\\nLoading pkl files ...')    \n",
    "    # train features\n",
    "    clustered_codebooks = []\n",
    "    train_feature_dict = {}\n",
    "    for index, descriptor in enumerate(descriptors_desired):\n",
    "        print(descriptor)\n",
    "        with open(FILEPATTERN_DESCRIPTOR_TRAIN.format(descriptor),'rb') as pkl_file_train:\n",
    "                train_feature_dict[descriptor] = pickle.load(pkl_file_train)\n",
    "        clustered_codebooks.append(helpers.createCodebook(train_feature_dict[descriptor], codebook_size=codebook_sizes[index]))\n",
    "\n",
    "    n_encoded_images_train = len(train_feature_dict[descriptors_desired[0]])\n",
    "        \n",
    "    train_data=[]\n",
    "    train_labels=[]\n",
    "    for i in range(n_encoded_images_train):\n",
    "        image_data = []\n",
    "        for index, descriptor in enumerate(descriptors_desired):            \n",
    "            bow_feature_vector = helpers.encodeImage(train_feature_dict[descriptor][i].data, clustered_codebooks[index])\n",
    "            image_data = np.concatenate((image_data, bow_feature_vector), axis=0)\n",
    "\n",
    "        train_data.append(image_data)\n",
    "        train_labels.append(train_feature_dict[descriptors_desired[0]][i].label)\n",
    "\n",
    "    print(f'Number of encoded train images: {n_encoded_images_train}')\n",
    "    \n",
    "    # test features\n",
    "    test_feature_dict = {}\n",
    "    for descriptor in descriptors_desired:\n",
    "        with open(FILEPATTERN_DESCRIPTOR_TEST.format(descriptor),'rb') as pkl_file_test:\n",
    "            test_feature_dict[descriptor] = pickle.load(pkl_file_test)\n",
    "\n",
    "    n_encoded_images_test = len(test_feature_dict[descriptors_desired[0]])        \n",
    "    \n",
    "    test_data=[]\n",
    "    for i in range(n_encoded_images_test):\n",
    "        image_data = []\n",
    "        for index, descriptor in enumerate(descriptors_desired):\n",
    "            bow_feature_vector = helpers.encodeImage(test_feature_dict[descriptor][i].data, clustered_codebooks[index])\n",
    "            image_data = np.concatenate((image_data, bow_feature_vector), axis=0)\n",
    "\n",
    "        test_data.append(image_data)\n",
    "        \n",
    "    print(f'Number of encoded test images: {n_encoded_images_test}')\n",
    "        \n",
    "    return train_data, train_labels, test_data\n",
    "\n",
    "def load_pkl_files_FV(descriptors_desired, mixture_sizes):\n",
    "    print('\\nLoading pkl files ...')    \n",
    "    # train features\n",
    "    clustered_mixtures = []\n",
    "    train_feature_dict = {}\n",
    "    for index, descriptor in enumerate(descriptors_desired):\n",
    "        print(descriptor)\n",
    "        with open(FILEPATTERN_DESCRIPTOR_TRAIN.format(descriptor),'rb') as pkl_file_train:\n",
    "            train_feature_dict[descriptor] = pickle.load(pkl_file_train)\n",
    "        clustered_mixtures.append(helpers.createGaussianMixture(train_feature_dict[descriptor], K_value=mixture_sizes[index]))\n",
    "\n",
    "    n_encoded_images_train = len(train_feature_dict[descriptors_desired[0]])\n",
    "        \n",
    "    train_data=[]\n",
    "    train_labels=[]\n",
    "    for i in range(n_encoded_images_train):\n",
    "        image_data = []\n",
    "        for index, descriptor in enumerate(descriptors_desired):            \n",
    "            bow_feature_vector = helpers.encodeImage_FV(train_feature_dict[descriptor][i].data, clustered_mixtures[index],gamma=0.1)\n",
    "            image_data = np.concatenate((image_data, bow_feature_vector), axis=0)\n",
    "\n",
    "        train_data.append(image_data)\n",
    "        train_labels.append(train_feature_dict[descriptors_desired[0]][i].label)\n",
    "\n",
    "    print(f'Number of encoded train images: {n_encoded_images_train}')\n",
    "    \n",
    "    # test features\n",
    "    test_feature_dict = {}\n",
    "    for descriptor in descriptors_desired:\n",
    "        with open(FILEPATTERN_DESCRIPTOR_TEST.format(descriptor),'rb') as pkl_file_test:\n",
    "            test_feature_dict[descriptor] = pickle.load(pkl_file_test)\n",
    "\n",
    "    n_encoded_images_test = len(test_feature_dict[descriptors_desired[0]])        \n",
    "    \n",
    "    test_data=[]\n",
    "    for i in range(n_encoded_images_test):\n",
    "        image_data = []\n",
    "        for index, descriptor in enumerate(descriptors_desired):\n",
    "            bow_feature_vector = helpers.encodeImage_FV(test_feature_dict[descriptor][i].data, clustered_mixtures[index],gamma=0.1)\n",
    "            image_data = np.concatenate((image_data, bow_feature_vector), axis=0)\n",
    "\n",
    "        test_data.append(image_data)\n",
    "        \n",
    "    print(f'Number of encoded test images: {n_encoded_images_test}')\n",
    "        \n",
    "    return train_data, train_labels, test_data\n",
    "\n",
    "def encode_labels(train_labels):\n",
    "    print('\\nEncoding labels ...')\n",
    "    return LabelEncoder().fit_transform(train_labels)\n",
    "\n",
    "def plot_histogram_train_labels(train_labels, n_classes):\n",
    "    # plot histogram of train labels\n",
    "    labelfreq, bins = np.histogram(train_labels, bins=range(n_classes + 1))\n",
    "    plt.figure()\n",
    "    plt.bar(range(n_classes), labelfreq)\n",
    "    plt.show()\n",
    "    print(f'Label frequencies = {labelfreq}')\n",
    "    priors = labelfreq/np.linalg.norm(labelfreq)\n",
    "    print(f'Priors = {priors}')\n",
    "    return labelfreq, priors\n",
    "\n",
    "def split_train_test_data(train_data, train_labels):\n",
    "    print('\\nSplitting train and test data ...')\n",
    "    return train_test_split(train_data, train_labels, test_size=0.25, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJLukJcYMH4w",
    "outputId": "55d1db0a-48c3-4de3-da9c-7795dc4293b7"
   },
   "outputs": [],
   "source": [
    "#descriptors_desired=['sift', 'surf', 'freak', 'vgg', 'boost_desc', 'daisy', 'lucid', 'orb']\n",
    "descriptors_desired=['sift', 'freak', 'boost_desc', 'daisy', 'lucid']\n",
    "#codebook_sizes = [400, 400, 400, 400, 400, 400, 400, 400]\n",
    "mixture_sizes = [7, 7, 7, 7, 7, 7]\n",
    "\n",
    "#extract_features()\n",
    "#train_data, train_labels, test_data = load_pkl_files_FV(descriptors_desired, mixture_sizes)\n",
    "#train_labels = encode_labels(train_labels)\n",
    "#labelfreq, priors = plot_histogram_train_labels(train_labels, n_classes)\n",
    "\n",
    "#x_train, x_test, r_train, r_test = split_train_test_data(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-Fa4fNPMH5A"
   },
   "outputs": [],
   "source": [
    "def dump_data(train_data, train_labels, test_data):\n",
    "    print('\\nDumping data ...')\n",
    "    train_data_out = open(\"train_data_FV.pickle\", \"wb\")\n",
    "    pickle.dump(train_data, train_data_out)\n",
    "    train_labels_out = open(\"train_labels_FV.pickle\", \"wb\")\n",
    "    pickle.dump(train_labels, train_labels_out)\n",
    "    test_data_out = open(\"test_data_FV.pickle\", \"wb\")\n",
    "    pickle.dump(test_data, test_data_out)\n",
    "    \n",
    "def load_data():\n",
    "    print('\\nLoading data ...')\n",
    "    train_data_in = open(\"train_data.pickle\", \"rb\")\n",
    "    train_data = pickle.load(train_data_in)\n",
    "    train_labels_in = open(\"train_labels.pickle\", \"rb\")\n",
    "    train_labels = pickle.load(train_labels_in)\n",
    "    test_data_in = open(\"test_data.pickle\", \"rb\")\n",
    "    test_data = pickle.load(test_data_in)\n",
    "    return train_data, train_labels, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "xjrRhPH5MH5M",
    "outputId": "cfb694cc-6bc6-48aa-ad77-28337d87ac02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data ...\n",
      "\n",
      "Splitting train and test data ...\n"
     ]
    }
   ],
   "source": [
    "#dump_data(train_data, train_labels, test_data)\n",
    "train_data, train_labels, test_data = load_data()\n",
    "#plot_histogram_train_labels(train_labels, n_classes)\n",
    "x_train, x_test, r_train, r_test = split_train_test_data(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jjFDsbIuMH5b"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk0oVSGtMH5f"
   },
   "outputs": [],
   "source": [
    "# pca\n",
    "def do_pca(data, labels, pca_components=None, pca_threshold=0.9):\n",
    "    print('\\nDoing PCA ...')\n",
    "    normalizer = Normalizer()\n",
    "    pca = PCA(n_components=pca_components, whiten=True)\n",
    "    pipe = Pipeline(steps=[('normalizer', normalizer),\n",
    "                           ('pca', pca)])\n",
    "    pipe.fit(data, labels)\n",
    "    \n",
    "    if pca_components == None:\n",
    "        plt.plot(np.cumsum((pca.explained_variance_ratio_)))\n",
    "        plt.axhline(y=pca_threshold, color='r', linestyle='-')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        pca_components = int(np.argmax(np.cumsum(pca.explained_variance_ratio_) > pca_threshold))\n",
    "        print(f'Number of PCA components = {pca_components}')\n",
    "    return pca_components\n",
    "\n",
    "def do_lda(data, labels, priors, lda_components=None):\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(data, labels)\n",
    "    \n",
    "    if lda_components == None:\n",
    "        plt.plot(np.cumsum((lda.explained_variance_ratio_)))\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        lda_components = len(lda.explained_variance_ratio_)\n",
    "        print(f'Number of LDA components = {lda_components}')\n",
    "    return lda_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "uuqxX65fMH5s",
    "outputId": "6d78a7c0-b6a1-42cd-cc82-65ac1fdb421d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing PCA ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PCA components = 233\n"
     ]
    }
   ],
   "source": [
    "pca_threshold = 0.6\n",
    "pca_components = None\n",
    "#lda_components = None\n",
    "\n",
    "pca_components = do_pca(x_train, r_train, pca_threshold=pca_threshold)\n",
    "#lda_components = do_lda(x_train, r_train, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYLjpdHYMH58",
    "outputId": "f4161314-a4ce-4bea-898d-8f503e40368a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ixn_IcjMH6L"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9977ZtoMH6Q"
   },
   "outputs": [],
   "source": [
    "def create_model(train_data, train_labels, test_data, pca_components, tuned_parameters):\n",
    "    # grid search\n",
    "    # preprocessing\n",
    "    normalizer = Normalizer()\n",
    "    #pca = PCA(n_components=pca_components, whiten=True)\n",
    "    kernel_pca = KernelPCA(n_components=pca_components, n_jobs=-1)\n",
    "    #lda = LinearDiscriminantAnalysis(shrinkage=True)\n",
    "    #selector = RFECV(model, step=0.4, cv=5)\n",
    "    \n",
    "    # create model and pipeline\n",
    "    kernel_gpc = 10*RBF()\n",
    "    model = GaussianProcessClassifier(kernel=kernel_gpc, multi_class=\"one_vs_rest\", max_iter_predict=1000, n_restarts_optimizer=3)\n",
    "    pipe = Pipeline(steps=[('normalizer', normalizer),\n",
    "                          #('pca', pca),\n",
    "                           ('kernel_pca', kernel_pca),\n",
    "                          #('lda', lda),\n",
    "                           ('model', model)])\n",
    "    \n",
    "    # grid search with k-fold cross-validation\n",
    "    grid_search = GridSearchCV(pipe, tuned_parameters, scoring='neg_log_loss', iid=False, cv=5, n_jobs=-1, refit=True, verbose=10)\n",
    "    grid_search.fit(train_data, train_labels)\n",
    "    \n",
    "    print(f'Best parameters = {grid_search.best_params_}')\n",
    "    print(f'Best score = {grid_search.best_score_}')\n",
    "    \n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "        print(f\"{mean:0.3f} (+/-{std*2:0.03f}) for {params}\")\n",
    "        \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "NjEmJyZGMH6d",
    "outputId": "78819a34-fd14-4da4-a85d-2aa60da16226",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "#kernel1 = 10 * RBF(length_scale=0.1) #[1,1] 1.31,\n",
    "#kernel12 = 10 * RBF(length_scale=0.5) #[10,1] 1.26\n",
    "#kernel13 = 10 * RBF(length_scale=10)\n",
    "#kernel2 = 1*RationalQuadratic(length_scale=1, alpha=1) #[1,1] 1.48, [0.3,2] worse, [2,1] 1.48\n",
    "#kernel21 = 5*RationalQuadratic(length_scale=1, alpha=1)\n",
    "#kernel22 = 10*RationalQuadratic(length_scale=1, alpha=1)\n",
    "#kernel3 = Matern(length_scale=1.0) #[1]1.53, [2]\n",
    "#kernel4 = 1*ExpSineSquared(length_scale=1.0, periodicity=1.0)\n",
    "\n",
    "tuned_parameters = [{'model__kernel__k2__length_scale': [1.0],\n",
    "                     #'model__kernel__alpha': [1, 5, 10],\n",
    "                     #'model__kernel': [kernel2],\n",
    "                    }\n",
    "                   ]\n",
    "\n",
    "pipe = create_model(x_train, r_train, x_test, pca_components, tuned_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYatuqW_MH6t"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BbiovfkMH6x"
   },
   "outputs": [],
   "source": [
    "def do_predictions(pipe, test_data):\n",
    "    return pipe.predict(test_data), pipe.predict_proba(test_data)\n",
    "    \n",
    "def print_scores(pipe, train_data, train_labels, test_data, test_labels, pred_proba):\n",
    "    print(f\"logloss = {log_loss(test_labels, pred_proba):.2f}\")\n",
    "    print(f\"train   = {pipe.score(train_data, train_labels)*100:.1f} %\")\n",
    "    print(f\"test    = {pipe.score(test_data, test_labels)*100:.1f} %\")\n",
    "    # uses StratifiedKFold\n",
    "    scores = cross_val_score(pipe, x_train, r_train, cv=5)\n",
    "    print(f\"CVA     = {scores.mean()*100:.1f} %\")\n",
    "    print(f\"CVS     = {scores.std()*100:.2f} %\")\n",
    "\n",
    "def plot_learning_curve(pipe, data, labels):\n",
    "    plt.figure()\n",
    "    # uses StratifiedKFold\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        pipe, data, labels, train_sizes=np.linspace(.1, 1.0, 5), cv=5, scoring='neg_log_loss', n_jobs=-1, verbose=10)\n",
    "    train_scores_mean = -1*np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -1*np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "def show_confusion_matrix(labels, prediction):\n",
    "    cm = confusion_matrix(labels, prediction)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plt.imshow(cm)\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfWLXH0fMH69"
   },
   "outputs": [],
   "source": [
    "pred, pred_proba = do_predictions(pipe, x_test)\n",
    "print_scores(pipe, x_train, r_train, x_test, r_test, pred_proba)\n",
    "#plot_learning_curve(pipe, x_train, r_train)\n",
    "show_confusion_matrix(r_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PLdAXghMH7T"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_QM9UgRTMH7c"
   },
   "outputs": [],
   "source": [
    "def make_submission(pipe, train_data, train_labels, test_data, label_strings):\n",
    "    print('\\nMaking submission ...')\n",
    "    pipe.fit(train_data, train_labels)\n",
    "    predict_proba = pipe.predict_proba(test_data)\n",
    "\n",
    "    pred_file_path = os.path.join(PREDICTION_PATH, helpers.generateUniqueFilename('pred', 'csv'))\n",
    "    helpers.writePredictionsToCsv(predict_proba, pred_file_path, label_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "31MVb2W3MH7x",
    "outputId": "120e58cc-0cd4-4b41-c33b-66d89a757728"
   },
   "outputs": [],
   "source": [
    "make_submission(pipe, train_data, train_labels, test_data, label_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Non-linear_Model.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
